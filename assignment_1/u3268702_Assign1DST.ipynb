{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "3. [Modeling](#Modeling)\n",
    "4. [Results](#Results)\n",
    "5. [Conclusion](#Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data\n",
    "\n",
    "df=pd.read_csv(\"data/zomato_df_final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the first few rows of the data\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all the column names\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the numerical and categorical variables\n",
    "\n",
    "num_var=df.select_dtypes(include=\"number\").columns\n",
    "cat_var=df.select_dtypes(exclude=\"number\").columns\n",
    "\n",
    "print(num_var)\n",
    "print(cat_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the NA values\n",
    "\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(), yticklabels=False,cbar=False,cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detecting outliers in data\n",
    "\n",
    "#list of numerical variables\n",
    "numerical_cols=df[num_var]\n",
    "\n",
    "#loop through each numerical columns to detect outliers\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "\n",
    "    #define the bound for ouliers\n",
    "    lower_bound= Q1 - 1.5*IQR\n",
    "    upper_bound= Q3 + 1.5*IQR\n",
    "\n",
    "    #outlier\n",
    "    outliers=df[(df[col] < lower_bound) | (df[col]> upper_bound)]\n",
    "\n",
    "    print(f\"Outliers in {col}:\")\n",
    "    print(outliers)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing outliers sing boxplot \n",
    "\n",
    "#creating subplot axes\n",
    "nrows=2\n",
    "ncols=3\n",
    "fig, axes =plt.subplots(nrows,ncols, figsize=(15,10))\n",
    "\n",
    "# Flatten the axes array to make it easier to iterate over\n",
    "axes = axes.flatten()\n",
    "\n",
    "# use to set style of background of plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define properties for the outliers (fliers)\n",
    "flierprops = dict(marker='o', markerfacecolor='red', markersize=8, linestyle='none')\n",
    "\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    sns.boxplot(y=df[col], ax=axes[i], color=\"skyblue\",flierprops=flierprops)\n",
    "    axes[i].set_title(f'Boxplot of {col}')\n",
    "\n",
    "plt.tight_layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many unique cuisines are served by Sydney restaurants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'cuisine' column is of string type\n",
    "df['cuisine'] = df['cuisine'].astype(str)\n",
    "\n",
    "# Now clean and split the cuisines\n",
    "df['cuisine'] = df['cuisine'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(',')\n",
    "\n",
    "# Use explode to flatten the list of cuisines into individual rows\n",
    "df_exploded = df.explode('cuisine')\n",
    "\n",
    "# Strip whitespace from cuisine names and count unique cuisines\n",
    "df_exploded['cuisine'] = df_exploded['cuisine'].str.strip()\n",
    "unique_cuisines = df_exploded['cuisine'].nunique()\n",
    "\n",
    "# Optional: List the unique cuisines\n",
    "cuisine_list = df_exploded['cuisine'].unique()\n",
    "\n",
    "print(f\"There are {unique_cuisines} unique cuisines served by Sydney restaurants.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_counts=df_exploded[\"cuisine\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plottong the top 15 cuisines offered by sydney restaurants\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = sns.color_palette(\"tab10\", n_colors=25)\n",
    "sns.barplot(y=cuisine_counts.index[:25], x=cuisine_counts.values[:25],palette=colors)  \n",
    "plt.title('Top 25 Most Popular Cuisines')\n",
    "plt.xlabel('Number of Restaurants')\n",
    "plt.ylabel('Cuisine')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which suburbs (top 3) have the highest number of restaurants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"subzone\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_suburb=df[\"subzone\"].value_counts()\n",
    "\n",
    "#getting the top 3 suburbs with the highest number of restaurants\n",
    "top_3_suburbs = restaurant_suburb.head(3)\n",
    "\n",
    "#result\n",
    "print(\"Top 3 suburbs with the highest number of restaurants:\")\n",
    "print(top_3_suburbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the top 3 suburbs\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.barplot(x=top_3_suburbs.index[:3],y=top_3_suburbs.values[:3],palette=\"tab10\")\n",
    "plt.title(\"Top 3 Suburbs with highest number of restaurants\")\n",
    "plt.xlabel(\"Suburbs\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “Restaurants with ‘excellent’ ratings are mostly costly while those with ‘Poor’ ratings are rarely expensive”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this if the above statement is true or not, first we will make a boxplot to see the cost distribution for each rating category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Create a boxplot to show the distribution of costs by rating\n",
    "sns.boxplot(x=df['rating_text'], y=df['cost'], palette=\"tab10\")\n",
    "\n",
    "plt.xlabel(\"Ratings\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Boxplot of Cost Distribution by Ratings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Use histplot with stacking and explicitly define legend=True to make sure legend is created\n",
    "sns.histplot(data=df, x=\"cost\", hue=\"rating_text\", kde=True, multiple=\"stack\",bins=30)\n",
    "\n",
    "# Limit x-axis for better focus on the range\n",
    "plt.xlim(0, 200)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Stacked Histogram of Cost by Rating', fontsize=16)\n",
    "plt.xlabel('Cost', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key observations:\n",
    "\n",
    "-\"**Excellent**\" ratings: These restaurants tend to have a higher median cost, with the boxplot showing that many of these restaurants fall in the higher cost range. The interquartile range (IQR) is higher compared to other ratings, indicating that restaurants with \"Excellent\" ratings tend to be more expensive on average.There are outliers above 200, indicating some restaurants with \"Excellent\" ratings have very high costs, with some reaching around 500. \n",
    "\n",
    "-\"**Poor**\" ratings:The median cost for \"Poor\" rated restaurants is 50, which is higher than \"Average\" rated restaurants.The IQR is slightly larger compared to \"Average\" rated restaurants, meaning there’s more cost variation among \"Poor\" rated restaurants.A few outliers show some \"Poor\" rated restaurants can have higher costs, but most are still relatively affordable.\n",
    "\n",
    "-\"**Very Good**\" ratings:The median cost for \"Very Good\" rated restaurants is lower than \"Excellent\" rated ones.The IQR is smaller, indicating less variation in restaurant costs. The box is mostly concentrated below 100, suggesting that most \"Very Good\" rated restaurants are moderately priced.There are fewer outliers compared to \"Excellent\" rated restaurants, but still a few higher-cost restaurants in this category.\n",
    "\n",
    "-\"**Good**\" ratings:The median cost is lower than \"Very Good\" and \"Excellent\" but still close to 50.The IQR is small, indicating that most \"Good\" rated restaurants are affordable, with costs primarily under 100.There are a few outliers above 200, but most restaurants are in a lower price range.\n",
    "\n",
    "-\"**Average**\" ratings:The median cost is close to 45, making it one of the lower-cost categories.The IQR is small, meaning that there’s little variation in the cost for \"Average\" rated restaurants.There are a few outliers, but the majority of restaurants are in the affordable range, under 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further quantify, we can create a summary statistics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for each rating category\n",
    "cost_summary = df.groupby('rating_text')['cost'].agg([\"mean\",\"median\"])\n",
    "\n",
    "print(\"Median cost per rating category:\")\n",
    "print(cost_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the boxplot and summary statistics, I agree with the statement. Restaurants with \"Excellent\" ratings are generally more expensive, as reflected by the higher median cost and the presence of more high-cost outliers. In contrast, \"Poor\" rated restaurants tend to have lower costs, with fewer high-cost outliers, supporting the idea that they are generally less expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating histogram to see the distribution of the data\n",
    "plt.figure(figsize=(9,8))\n",
    "ax=sns.histplot(data=df,x=\"cost\", bins=30, kde=True, color=\"orange\",edgecolor=\"red\")\n",
    "ax.lines[0].set_color(\"green\")\n",
    "plt.title(\"Distibution of Cost Variable\")\n",
    "plt.xlabel(\"Cost\")\n",
    "plt.xlim(0, 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Interpretation:\n",
    "\n",
    "- The histogram shows that the majority of restaurant costs are clustered between 20 and 50. \n",
    "- This can be observed from the tall bars in this range, with the highest peak around 25.\n",
    "- The distribution is right-skewed, meaning there are fewer restaurants with higher costs (over 100), as indicated by the long tail on the right side.\n",
    "- The green KDE curve (Kernel Density Estimate) overlays the histogram, providing a smooth representation of the probability density. It confirms that most restaurants have costs concentrated between 20 and 50.\n",
    "- There are some outliers in the dataset, as seen from the bars on the right-hand side that go beyond 100, with a few even reaching as high as 200. This indicates a small proportion of restaurants with much higher costs, but they are very uncommon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a countplot to show the count of each rounded rating number\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='rating_number', data=df, palette='Set2')\n",
    "plt.title('Count of Restaurants by Rating (0-5 Scale)')\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Number of Restaurants')\n",
    "plt.ylim(0, 1000) \n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Interpretation:\n",
    "\n",
    "- Most restaurants are rated between 3.0 and 3.5, suggesting that the majority are considered average to slightly above average in quality.\n",
    "- Extremely low or high ratings are relatively rare, meaning most restaurants avoid being rated as exceptionally bad or outstanding.\n",
    "- This distribution shows that customer ratings tend to cluster around the middle, and it's harder for restaurants to achieve exceptional or very poor ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"type\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'cuisine' column is of string type\n",
    "df['type'] = df['type'].astype(str)\n",
    "\n",
    "# Now clean and split the cuisines\n",
    "df['type'] = df['type'].str.strip(\"[]\").str.replace(\"'\", \"\").str.split(',')\n",
    "\n",
    "# Use explode to flatten the list of cuisines into individual rows\n",
    "df_exploded_2 = df.explode('type')\n",
    "\n",
    "# Strip whitespace from cuisine names and count unique cuisines\n",
    "df_exploded_2['type'] = df_exploded_2['type'].str.strip()\n",
    "#unique_cuisines = df_exploded['cuisine'].nunique()\n",
    "\n",
    "# Optional: List the unique cuisines\n",
    "#cuisine_list = df_exploded['cuisine'].unique()\n",
    "\n",
    "type_count=df_exploded_2[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(x=type_count.values[:15],y=type_count.index[:15],palette=\"tab10\")\n",
    "plt.title(\"Barplot of types of restaurant in Sydney\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Types of restaurants\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_model=df[[\"cost\",\"cuisine\",\"subzone\",\"type\",\"votes\",\"cost_2\",\"rating_number\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 7)\n"
     ]
    }
   ],
   "source": [
    "print(features_model.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_model=features_model.dropna(subset=[\"rating_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7184, 7)\n"
     ]
    }
   ],
   "source": [
    "print(features_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare source and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=features_model.drop([\"rating_number\"], axis=1)\n",
    "y=features_model[\"rating_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7184, 6)\n",
      "(7184,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5747, 6), (1437, 6))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cost', 'votes', 'cost_2'], dtype='object')\n",
      "Index(['cuisine', 'subzone', 'type'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "print(num_features)\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering missing values in numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost      float64\n",
      "votes     float64\n",
      "cost_2    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train[num_features].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost      83\n",
      "votes      0\n",
      "cost_2    83\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for missing values in train set\n",
    "print(X_train[num_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost      20\n",
      "votes      0\n",
      "cost_2    20\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for missing values in test set\n",
    "print(X_test[num_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost      0.01\n",
      "votes     0.00\n",
      "cost_2    0.01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# percentage of missing values in each variable\n",
    "print(round(X_train[num_features].isnull().mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for training set in numerical variables\n",
      "cost      0\n",
      "votes     0\n",
      "cost_2    0\n",
      "dtype: int64\n",
      "Result for test set in numerical variables\n",
      "cost      0\n",
      "votes     0\n",
      "cost_2    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#imputaion of missing values with the median value\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train[num_features]=imputer.fit_transform(X_train[num_features])\n",
    "X_test[num_features]=imputer.transform(X_test[num_features])\n",
    "\n",
    "print(\"Result for training set in numerical variables\")\n",
    "print(X_train[num_features].isnull().sum())\n",
    "\n",
    "print(\"Result for test set in numerical variables\")\n",
    "print(X_test[num_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleImputer with the median strategy has been used to handle missing values in the numerical columns. This approach ensures that missing values are replaced with the median calculated from the training data, which is robust to outliers.\n",
    "\n",
    "I have fitted the imputer on the training data to avoid data leakage, meaning there is no use of any information from the test set while training. After fitting the imputer, same median values have been applied to both the training and test data, ensuring consistency in how missing values are handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering missing values in categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuisine     0\n",
      "subzone     0\n",
      "type       19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking the missing values in of categorical variables in train set\n",
    "print(X_train[cat_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuisine    0\n",
      "subzone    0\n",
      "type       2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking the missing values in of categorical variables in test set\n",
    "print(X_test[cat_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleImputer with the \"most frequent\" strategy was used to handle missing values in categorical variables. This filled missing values with the most commonly occurring category (mode) in each column.\n",
    "\n",
    "The imputer was fitted on the training data and applied to both training and test sets, ensuring that the imputation was based on the training data only, preventing data leakage and maintaining consistency in handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = X_train.dropna(subset=['type'])\n",
    "X_test_clean = X_test.dropna(subset=['type'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of X_train after dropping rows with NaN in 'type': (5728, 6)\n",
      "New shape of X_test after dropping rows with NaN in 'type': (1435, 6)\n"
     ]
    }
   ],
   "source": [
    "# Check the new shape after dropping rows\n",
    "print(f\"New shape of X_train after dropping rows with NaN in 'type': {X_train_clean.shape}\")\n",
    "print(f\"New shape of X_test after dropping rows with NaN in 'type': {X_test_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuisine     0\n",
      "subzone     0\n",
      "type       19\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking the missing values in of categorical variables in train set\n",
    "print(X_train[cat_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the missing values in of categorical variables in test set\n",
    "print(X_test[cat_features].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering outliers in numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to replace outliers with maximum values based on the upper bound\n",
    "def max_value(df_temp, variable, upper_bound):\n",
    "    return np.where(df_temp[variable] > upper_bound, upper_bound, df_temp[variable])\n",
    "\n",
    "# Dictionary to hold the calculated upper bounds for each column\n",
    "cols_with_outliers = {}\n",
    "\n",
    "# List of numerical columns in the dataset\n",
    "numerical_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Calculate the IQR for each numerical column and store the upper bounds\n",
    "for col in numerical_cols:\n",
    "    Q1 = X_train[col].quantile(0.25)\n",
    "    Q3 = X_train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the upper bound as Q3 + 1.5 * IQR (standard IQR rule)\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Store the upper bound in the dictionary\n",
    "    cols_with_outliers[col] = upper_bound\n",
    "\n",
    "# Apply the upper bounds to cap outliers in both X_train and X_test\n",
    "for df_temp in [X_train, X_test]:\n",
    "    for col in cols_with_outliers:\n",
    "        df_temp[col] = max_value(df_temp, col, cols_with_outliers[col])\n",
    "\n",
    "# Now the outliers in both X_train and X_test have been capped based on the IQR method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use seaborn library to plot elegant ones\n",
    "df_custom = X_train[[\"cost\",\"votes\",\"cost_2\"]]\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = sns.boxplot(data=df_custom, orient=\"h\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "\n",
    "# Sample DataFrame with categorical columns\n",
    "# X_train and X_test assumed to have 'cuisine', 'subzone', and 'type' as columns\n",
    "\n",
    "# Step 1: Convert 'cuisine' and 'type' from string to list format\n",
    "# Using ast.literal_eval to safely convert strings to lists\n",
    "X_train['cuisine'] = X_train['cuisine'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "X_train['type'] = X_train['type'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "X_test['cuisine'] = X_test['cuisine'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "X_test['type'] = X_test['type'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Now, the rest of your process can continue as is, e.g., normalizing the lists, binarizing, and encoding\n",
    "\n",
    "\n",
    "# Step 2: Normalize the 'cuisine' and 'type' columns (lowercase and strip whitespace)\n",
    "X_train['cuisine'] = X_train['cuisine'].apply(lambda lst: [item.strip().lower() for item in lst])\n",
    "X_train['type'] = X_train['type'].apply(lambda lst: [item.strip().lower() for item in lst])\n",
    "\n",
    "X_test['cuisine'] = X_test['cuisine'].apply(lambda lst: [item.strip().lower() for item in lst])\n",
    "X_test['type'] = X_test['type'].apply(lambda lst: [item.strip().lower() for item in lst])\n",
    "\n",
    "# Step 3: Apply MultiLabelBinarizer to 'cuisine' and 'type'\n",
    "mlb_cuisine = MultiLabelBinarizer()\n",
    "mlb_type = MultiLabelBinarizer()\n",
    "\n",
    "# Fit on the train set and transform both the train and test sets\n",
    "X_train_cuisine_encoded = pd.DataFrame(mlb_cuisine.fit_transform(X_train['cuisine']), columns=mlb_cuisine.classes_, index=X_train.index)\n",
    "X_test_cuisine_encoded = pd.DataFrame(mlb_cuisine.transform(X_test['cuisine']), columns=mlb_cuisine.classes_, index=X_test.index)\n",
    "\n",
    "X_train_type_encoded = pd.DataFrame(mlb_type.fit_transform(X_train['type']), columns=mlb_type.classes_, index=X_train.index)\n",
    "X_test_type_encoded = pd.DataFrame(mlb_type.transform(X_test['type']), columns=mlb_type.classes_, index=X_test.index)\n",
    "\n",
    "# Step 4: Apply OneHotEncoder to 'subzone'\n",
    "onehot_encoder = OneHotEncoder(sparse=False, drop='first')  # Using drop='first' to avoid multicollinearity\n",
    "subzone_encoded_train = onehot_encoder.fit_transform(X_train[['subzone']])\n",
    "subzone_encoded_test = onehot_encoder.transform(X_test[['subzone']])\n",
    "\n",
    "# Convert the one-hot encoded data to DataFrame\n",
    "subzone_encoded_train_df = pd.DataFrame(subzone_encoded_train, columns=onehot_encoder.get_feature_names_out(['subzone']), index=X_train.index)\n",
    "subzone_encoded_test_df = pd.DataFrame(subzone_encoded_test, columns=onehot_encoder.get_feature_names_out(['subzone']), index=X_test.index)\n",
    "\n",
    "# Step 5: Concatenate the encoded columns back to the original data\n",
    "X_train_encoded = pd.concat([X_train, X_train_cuisine_encoded, X_train_type_encoded, subzone_encoded_train_df], axis=1)\n",
    "X_test_encoded = pd.concat([X_test, X_test_cuisine_encoded, X_test_type_encoded, subzone_encoded_test_df], axis=1)\n",
    "\n",
    "# Optionally, drop the original 'cuisine', 'type', and 'subzone' columns if no longer needed\n",
    "X_train_encoded.drop(columns=['cuisine', 'type', 'subzone'], inplace=True)\n",
    "X_test_encoded.drop(columns=['cuisine', 'type', 'subzone'], inplace=True)\n",
    "\n",
    "# Final shape check\n",
    "print(f\"Shape of X_train_encoded: {X_train_encoded.shape}\")\n",
    "print(f\"Shape of X_test_encoded: {X_test_encoded.shape}\")\n",
    "\n",
    "# Check the first few rows to confirm\n",
    "#print(X_train_encoded.head(3))\n",
    "#print(X_test_encoded.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numeric columns in X_train, and transform in X_test\n",
    "X_train[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "X_test[num_features] = scaler.transform(X_test[num_features])\n",
    "\n",
    "# Check the processed datasets\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_mae=mean_squared_error(y_train, y_train_pred)\n",
    "model_test_mae=mean_squared_error(y_test, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model performance for Training set')\n",
    "print(\"Mean Squared Error:\", format(model_train_mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "rmse = np.sqrt(model_test_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print results\n",
    "print(\"R-squared: %.4f\" % r_squared)\n",
    "print(\"Mean Squared Error: %.4f\" % score)\n",
    "print(\"Root Mean Squared Error: %.4f\" % rmse) \n",
    "print(\"\\n\")\n",
    "print(\"-----------------------------------------------\")\n",
    "print(\"\\n\")\n",
    "print('Model performance for Testing set')\n",
    "print(\"Mean Squared Error:\", format(model_test_mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
